{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f50ad585-f7e1-4a14-a248-d7986cd9f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Code written by William Bidle and Ilana Zane '''\n",
    "\n",
    "__version__ = 'dev'\n",
    "\n",
    "##########################################################################################\n",
    "######################################## Imports #########################################\n",
    "##########################################################################################\n",
    "\n",
    "''' For Array Manipulation '''\n",
    "import numpy as np\n",
    "\n",
    "''' For Visualization '''\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "''' For Mathematical Expressions '''\n",
    "from sympy import * \n",
    "\n",
    "''' For Runtime/Progress Checking '''\n",
    "from tqdm import tqdm\n",
    "\n",
    "''' For File Reading/Writing '''\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "\n",
    "class NN:  \n",
    "    def __init__(self, layer_sequence = [], loss_function = 'MSE'):\n",
    "        \n",
    "        ''' \n",
    "        ##################################################################\n",
    "                    Initialize the Neural Network Class\n",
    "        ##################################################################\n",
    "\n",
    "        INPUTS:\n",
    "        - layer_sequence (list) : a list containing the nodes per layer and correcponding activation functions between layers\n",
    "        - loss_function (string) : the desired loss function to be used \n",
    "\n",
    "        OUTPUTS:\n",
    "        - None\n",
    "\n",
    "        ##################### Example ######################\n",
    "\n",
    "        layer_sequence = [1,'ReLU', 2]\n",
    "        loss_function = 'MSLE'\n",
    "        \n",
    "        nn = NN(layer_sequence, loss_function)\n",
    "\n",
    "        # Avaliable Class Properties\n",
    "        print('activation func library:\\n', nn.activation_funcs_library)\n",
    "        print()\n",
    "        print('loss func library:\\n', nn.loss_funcs_library)\n",
    "        print()\n",
    "        print('current weights:\\n', nn.weights)\n",
    "        print()\n",
    "        print('current activation functions:\\n', nn.activation_funcs)\n",
    "        print()\n",
    "        print('current loss function:\\n', nn.loss_func_label, ':', nn.loss_func)\n",
    "        print()\n",
    "        print('traing error:\\n', nn.training_err)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.activation_funcs_library = self.__load_func_libraries('activation_funcs_library.txt') # load in the function library        \n",
    "        \n",
    "        layers = layer_sequence[::2] # separate out the layer information\n",
    "        if all(isinstance(item, int) for item in layers) == True: # check that the declared number of nodes is an integer\n",
    "            layers = np.array(layer_sequence[::2], dtype = int) # set the *layers* property\n",
    "            self.weights = self.__initialize_weights(layers) # initialize the *weights* property based off of the desired layer sequence\n",
    "        else:\n",
    "            raise Exception('Invalid Layer Sequence!') # raise an exception if the input layer sequence is improperly defined\n",
    "            \n",
    "        activation_funcs = layer_sequence[1::2] # separate out the activation function information\n",
    "        if all(isinstance(item, str) for item in activation_funcs) == True: # check that the activation functions are strings\n",
    "            self.activation_funcs = [] # initialize the *activation_funcs* property\n",
    "            for i in range(len(activation_funcs)): # initialize each declared activation functions between layers\n",
    "                self.activation_funcs.append(self.__init_func(self.activation_funcs_library, activation_funcs[i]))\n",
    "\n",
    "        else:\n",
    "            raise Exception('Invalid Layer Sequence!') # raise an exception if the input layer sequence is improperly defined\n",
    "            \n",
    "            \n",
    "        self.loss_funcs_library = self.__load_func_libraries('loss_funcs_library.txt') # initialize the *loss_funcs_library* property\n",
    "        self.loss_func = self.__init_func(self.loss_funcs_library, loss_function) # initialize the *loss_func* property\n",
    "        self.loss_func_label = loss_function # initialize the *loss_func_label* property (used in plotting for now)\n",
    "        \n",
    "        self.training_err = None # initialize the *training_err* property (will be set later once the model is trained)\n",
    "\n",
    "    ##########################################################################################\n",
    "    #################################### Private Methods #####################################\n",
    "    ##########################################################################################\n",
    "    \n",
    "    def __load_func_libraries(self, func_file):\n",
    "    \n",
    "        ''' \n",
    "        ##################################################################\n",
    "            A method to load in dictionaries of available functions\n",
    "        ##################################################################\n",
    "\n",
    "        INPUTS:\n",
    "        - func_file (string) : the filename containing the library of usable functions\n",
    "\n",
    "        OUTPUTS:\n",
    "        - func_library (dict) : a dictionary of the usable functions\n",
    "        \n",
    "        '''\n",
    "\n",
    "        with open(func_file) as f: # open the desired file\n",
    "            data = f.read()\n",
    "      \n",
    "        func_library = json.loads(data) # reconstruct the data as a dictionary\n",
    "        \n",
    "        return func_library\n",
    "        \n",
    "    ##########################################################################################\n",
    "\n",
    "    def __init_func(self, func_library, func_name):\n",
    "\n",
    "        ''' \n",
    "        ##################################################################\n",
    "                Initialize a function from a function library\n",
    "        ##################################################################\n",
    "\n",
    "        INPUTS:\n",
    "        - func_library (dict) : a dictionary of the usable functions\n",
    "        - func_name (string) : the name of the mathematical function to be initialized (e.g., 'sigmoid')\n",
    "\n",
    "        OUTPUTS:\n",
    "        - expression (sympy.core.symbol.Symbol) : symbolic mathematical representation of 'func_name'\n",
    "\n",
    "        '''\n",
    "\n",
    "        try: # try to initialize the function\n",
    "            expression = func_library[func_name]\n",
    "            \n",
    "        except: # if the function doesn't exist within the function library, return an exception\n",
    "            raise Exception(\"Desired function '%s' does not exist within the 'func_library.'\" % func_name)\n",
    "            \n",
    "        x, y, y_hat = symbols('x y y_hat', real = True) # declare the variables of interest (x for activations, y and y_hat for loss)\n",
    "        expression = parse_expr(expression, local_dict = {'x': x, 'y': y, 'y_hat': y_hat}) # parse throught the expression\n",
    "        \n",
    "        return expression\n",
    "\n",
    "    ##########################################################################################\n",
    "\n",
    "    def __eval_func(self, expression, vals, diff = False):\n",
    "\n",
    "        ''' \n",
    "        ##################################################################\n",
    "                        Initialize an activation function\n",
    "        ##################################################################\n",
    "\n",
    "        INPUTS:\n",
    "        - expression (sympy.core.symbol.Symbol) : symbolic mathematical representation of 'func'\n",
    "        - vals (2D list) : the values to evaluate 'expression' with\n",
    "            |\n",
    "             ---> 1 sub-list for activation functions (x information), 2 sub-lists for loss functions (y, y_hat information)\n",
    "        - diff (Boolean) : whether or not to evaluate the derivitive of 'expression' at 'vals\n",
    "        \n",
    "        OUTPUTS:\n",
    "        - result (if diff = False ---> Float, if diff = True ---> NumPy 1D array) : evaluation of 'expression' at '_input_'                   \n",
    "\n",
    "        '''        \n",
    "        \n",
    "        if expression in self.activation_funcs: # Evaluate Activation Functions\n",
    "            x = Symbol('x', real = True) # the variable of interest\n",
    "\n",
    "            if diff == True: # differentiate only if the 'diff' flag is True\n",
    "                expression = expression.diff(x)\n",
    "\n",
    "            func = lambdify(x, expression) # allow the function to be evaluated from lists\n",
    "            result = func(vals[0]) # evaluate the function at the given input\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        \n",
    "        else: # Evaluate Loss Functions\n",
    "            y, y_hat = symbols('y, y_hat', real = True) # the variables of interest\n",
    "            \n",
    "            if diff == True: # differentiate only if the 'diff' flag is True\n",
    "                expression = expression.diff(y)\n",
    "                \n",
    "                func = lambdify((y, y_hat), expression) # allow the function to be evaluated from lists\n",
    "                result = func(vals[0], vals[1]) # evaluate the function at the given input\n",
    "                return result\n",
    "            \n",
    "            func = lambdify((y, y_hat), expression) # allow the function to be evaluated from lists\n",
    "            result = func(vals[0], vals[1]) # evaluate the function at the given input\n",
    "            return sum(result)\n",
    "    \n",
    "    ##########################################################################################\n",
    "    \n",
    "    def __initialize_weights(self,layers):\n",
    "        \n",
    "        ''' \n",
    "        ##################################################################\n",
    "                    Initialize the weights of the network \n",
    "        ##################################################################\n",
    "\n",
    "        INPUTS:\n",
    "        - layers (1D Numpy Array) : an array containing the layer information of the network\n",
    "        \n",
    "        OUTPUTS:\n",
    "        - weights (3D list) : list containing the (potentially different sized) 2D weight arrays between the different layers       \n",
    "\n",
    "        '''        \n",
    "        \n",
    "        layers_reorganized = np.flip(layers.repeat(2)[1:-1].reshape(len(layers)-1,2), axis = 1)\n",
    "\n",
    "        weights = [] # initialize the list of the weights between different layers\n",
    "        for i in range(len(layers_reorganized)):\n",
    "            weight = np.random.randn(layers_reorganized[i][0], \n",
    "                                     layers_reorganized[i][1] + 1) # include bias vector with the '+ 1'\n",
    "            weights.append(weight*np.sqrt(2/layers_reorganized[i][1])) # HE initialization for weights\n",
    "\n",
    "        return weights\n",
    "        \n",
    "    ##########################################################################################\n",
    "\n",
    "    def __update_weights(self, weights, layer_values, _label_):\n",
    "        \n",
    "        ''' \n",
    "        ##################################################################\n",
    "                        Update the weights of the network \n",
    "        ##################################################################\n",
    "\n",
    "        INPUTS:\n",
    "        - weights (3D list) : list containing the (potentially different sized) 2D weight arrays between the different layers \n",
    "        - layer_values (2D list) : list containing the values of each layer for a given input value (see method 'get_network_outputs' below)\n",
    "        - _label_ (1D NumPy Array) : array representation for the current label (usually one hot ecoded, see method 'One_Hot_Encode' below)\n",
    "        \n",
    "        OUTPUTS:\n",
    "        - weight_updates, weights (3D list) : list containing the updated 2D weight arrays between the different layers\n",
    "        - weights (3D list) : list containing the original 2D weight arrays between the different layers       \n",
    "\n",
    "        '''  \n",
    "    \n",
    "        activations = self.activation_funcs # get the list of desired activation functions\n",
    "\n",
    "        ''' Debug '''\n",
    "        # print('layer_values:', layer_values)\n",
    "        # print('weights:', weights)\n",
    "        # print()\n",
    "\n",
    "        weight_updates = weights.copy() # make a copy of the weights so they aren't changed\n",
    "\n",
    "        blue = np.diag(self.__eval_func(self.loss_func, [layer_values[-1], _label_], diff = True)) # blue in notes\n",
    "        \n",
    "        ''' Debug '''\n",
    "        # print('blue:', blue.shape)\n",
    "        # print()\n",
    "\n",
    "        layer_output = np.dot(weights[-1],np.concatenate((layer_values[-2], [1]))) # need to add an extra component to input for bias\n",
    "        red = self.__eval_func(activations[-1], [layer_output], diff = True) # red in notes\n",
    "        \n",
    "        ''' Debug '''\n",
    "        # print('red:', red.shape)\n",
    "        # print()\n",
    "\n",
    "        \n",
    "        for i in range(len(weights), 0, -1): # index through each weight (work backwards)\n",
    "            ''' Debug '''\n",
    "            # print('index i:', i - 1)\n",
    "\n",
    "            pink = np.concatenate((layer_values[i-1], [1])) # pink in notes \n",
    "            ''' Debug '''\n",
    "            # print('pink:', pink.shape)\n",
    "            # print()\n",
    "\n",
    "            grad = np.matmul(blue, np.outer(red, pink)) # first two terms in gradient\n",
    "            ''' Debug '''\n",
    "            # print('grad1:', grad.shape)\n",
    "            # print()\n",
    "\n",
    "            for j in range(len(weights), i, -1): # look forwards through each weight (only if there are forward weights)\n",
    "                ''' Debug '''\n",
    "                # print('index j:', j - 1)\n",
    "\n",
    "                orange = np.transpose(weights[j-1]) # orange in notes\n",
    "                ''' Debug '''\n",
    "                # print('orange:', orange.shape)\n",
    "                # print()\n",
    "\n",
    "                green = np.diag(self.__eval_func(activations[j-1], [np.dot(weights[j-2],np.concatenate((layer_values[j-2], [1])))], diff = True)) # green in notes\n",
    "                bias_vec = np.ones((len(green),1)) # add on the bias vector to make sure dimensions work properly\n",
    "                green = np.hstack((green,bias_vec)) # incorperate the bias\n",
    "                ''' Debug '''\n",
    "                # print('green:', green.shape)\n",
    "                # print()\n",
    "\n",
    "                grad = np.matmul(green,np.matmul(orange,grad)) # now multiply the rest to grad\n",
    "                ''' Debug '''\n",
    "                # print('grad2:', grad.shape)\n",
    "                # print()\n",
    "\n",
    "            weight_updates[i-1] = grad # record the change in weight\n",
    "\n",
    "        return weight_updates, weights\n",
    "    \n",
    "    ##########################################################################################\n",
    "    #################################### Public Methods ######################################\n",
    "    ##########################################################################################\n",
    "    \n",
    "    def get_network_outputs(self, weights, _input_, _label_):\n",
    "        \n",
    "        activations = self.activation_funcs # get the list of desired activation functions\n",
    "        \n",
    "        current_layer = _input_\n",
    "        network_outputs = [current_layer] # add the first layer to the list\n",
    "\n",
    "        for i in range(len(weights)):\n",
    "\n",
    "            layer_output = np.dot(weights[i],np.concatenate((current_layer, [1]))) # need to add an extra component to input for bias\n",
    "            current_layer = self.__eval_func(activations[i], [layer_output], diff = False)\n",
    "            \n",
    "            network_outputs.append(current_layer)\n",
    "     \n",
    "        error = self.__eval_func(self.loss_func, [current_layer, _label_])\n",
    "\n",
    "        return network_outputs, error\n",
    "\n",
    "    ##########################################################################################\n",
    "    \n",
    "    def train(self, x_train, y_train, batch_size = 1, epochs = 1, epsilon = 1, visualize = False):\n",
    "            \n",
    "        weights = self.weights # get the list of weights \n",
    "\n",
    "        error_list = []\n",
    "\n",
    "        counter = 0 # keep track of the current iteration\n",
    "\n",
    "        weights_list = {} # create a dictionary to keep track of the weight updates (batch size)\n",
    "        for i in range(len(weights)):\n",
    "            weights_list[i] = [] # just a temporary blank array since training hasn't begun yet\n",
    "        \n",
    "        #create epochs \n",
    "        \n",
    "        for i in range(epochs):\n",
    "        \n",
    "        \n",
    "            # the training\n",
    "            for _input_, _label_ in tqdm(zip(x_train, y_train), total = len(x_train), desc = 'Epoch %s'%str(i+1)): # iterate through the inputs and labels\n",
    "\n",
    "                network_output, error = self.get_network_outputs(weights, _input_, _label_) # the current network output\n",
    "\n",
    "                weight_updates, weights = self.__update_weights(weights, network_output, _label_)\n",
    "\n",
    "                for j in range(len(weights)):\n",
    "                    weights_list[j].append(weight_updates[j])\n",
    "\n",
    "                counter += 1\n",
    "\n",
    "                if (counter) % batch_size == 0: \n",
    "                    for j in range(len(weights)):\n",
    "\n",
    "                        weights[j] -= epsilon*np.average(np.array(weights_list[j]), axis = 0)\n",
    "                        weights_list[j] = []\n",
    "\n",
    "                error_list.append(error)\n",
    "\n",
    "            self.weights = weights\n",
    "            self.training_err = error_list\n",
    "        \n",
    "        if visualize == True:\n",
    "            fig, ax = plt.subplots(figsize = (12,6))\n",
    "            \n",
    "            ax.plot(self.training_err) # to visualize error over time\n",
    "            \n",
    "            ax.set_xlabel('Training Sample', fontsize = 14)\n",
    "            ax.set_ylabel('%s Error' % self.loss_func_label, fontsize = 14)\n",
    "            \n",
    "            ax.grid(linestyle = '--')\n",
    "            \n",
    "            plt.show()\n",
    "    \n",
    "    ##########################################################################################\n",
    "    \n",
    "    def evaluate(self, x_test, y_test):\n",
    "\n",
    "        num_correct, num_wrong = 0, 0\n",
    "\n",
    "        for _input_, _label_ in tqdm(zip(x_test, y_test), desc = 'Evaluating Test Data', total = len(x_test)):\n",
    "            network_output, error = self.get_network_outputs(self.weights, _input_, _label_) # the current network output\n",
    "\n",
    "            ''' Debug '''\n",
    "\n",
    "            if np.argmax(network_output[-1]) == np.argmax(_label_):\n",
    "                num_correct += 1\n",
    "\n",
    "            else:\n",
    "                num_wrong += 1\n",
    "\n",
    "        print(\"% Correct:\", 100*num_correct/len(x_test))\n",
    "        print(\"% Wrong:\", 100*num_wrong/len(x_test))\n",
    "        \n",
    "    def save_model(self, filename = 'Saved_Model'):\n",
    "\n",
    "        save_path = 'Saved Models/' + filename\n",
    "        \n",
    "        to_save = [self.weights, self.activation_funcs] # save both the activations and weights\n",
    "        with open(save_path, \"wb\") as fp: # save the weights and activations\n",
    "            pickle.dump(to_save, fp)\n",
    "            \n",
    "        print()\n",
    "        print('Model saved at %s' % save_path)\n",
    "        print()\n",
    "        \n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    \n",
    "##########################################################################################\n",
    "################################## Other Useful Methods ##################################\n",
    "##########################################################################################\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "def One_Hot_Encode(labels):\n",
    "    num_unique_elements = np.unique(labels)\n",
    "    \n",
    "    dic = {}\n",
    "    counter = 0\n",
    "    for i in num_unique_elements:\n",
    "        if i in dic:\n",
    "            pass\n",
    "        else:\n",
    "            dic[i] = counter\n",
    "            counter += 1\n",
    "            \n",
    "    encoded_labels = np.zeros((len(labels), len(num_unique_elements)))\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        encoded_labels[i][dic[labels[i]]] = 1.\n",
    "        \n",
    "    return encoded_labels\n",
    "\n",
    "def load_model(filename = 'Saved_Model'):\n",
    "\n",
    "    load_path = 'Saved Models/' + filename\n",
    "    \n",
    "    with open(load_path, \"rb\") as fp: # Unpickling\n",
    "        loaded = pickle.load(fp)\n",
    "\n",
    "    weights = loaded[0]\n",
    "    activations = loaded[1]\n",
    "    \n",
    "    nn = NN() # initialize a blank Neural Network\n",
    "    nn.weights = weights\n",
    "    nn.activation_funcs = activations\n",
    "\n",
    "    return nn\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ca1dfd-c8d2-44d0-a558-b0749359f808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1426585-140c-4f77-9fd2-07d6d2a08591",
   "metadata": {},
   "source": [
    "# Softmax Developmemnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e0be0e7-85d9-470d-a43e-04f719b82ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8349bd16-a26a-4ff2-bf7c-9eccc1c6a55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{e^{{x}_{1}}}{e^{{x}_{1}} + e^{{x}_{2}}}$"
      ],
      "text/plain": [
       "exp(x[1])/(exp(x[1]) + exp(x[2]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{e^{{x}_{1} + {x}_{2}}}{2 e^{{x}_{1} + {x}_{2}} + e^{2 {x}_{1}} + e^{2 {x}_{2}}}$"
      ],
      "text/plain": [
       "exp(x[1] + x[2])/(2*exp(x[1] + x[2]) + exp(2*x[1]) + exp(2*x[2]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'Symbol' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-096690ee8fa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlambdify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndexed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<lambdifygenerated-1>\u001b[0m in \u001b[0;36m_lambdifygenerated\u001b[0;34m(Dummy_167)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_lambdifygenerated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDummy_167\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDummy_167\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDummy_167\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Symbol' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "x, n, i, j = symbols('x, n, i, j')\n",
    "s = exp(Indexed('x',1))/Sum(exp(Indexed('x',i)),(i,1,2)).doit()\n",
    "display(s)\n",
    "\n",
    "diff = simplify(s.diff(Indexed('x',1)))\n",
    "display(diff)\n",
    "\n",
    "f = lambdify(Indexed('x',1), s)\n",
    "f(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7940ce2-5011-4f23-a2dc-134a9cbae5bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{e^{{x}_{j}}}{\\sum_{i=0}^{n} e^{{x}_{i}}}$"
      ],
      "text/plain": [
       "exp(x[j])/Sum(exp(x[i]), (i, 0, n))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-123bffa590a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndexed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mSum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndexed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "x, i, n = symbols(\"x i n\")\n",
    "s = exp(Indexed('x',j))/Sum(exp(Indexed('x',i)),(i,0,n))\n",
    "display(s)\n",
    "b = np.array([1, 2, 3])\n",
    "s = s.subs(n, len(b)-1)\n",
    "\n",
    "display(s.doit())\n",
    "\n",
    "# f = lambdify(x, s)\n",
    "# print(f(b))\n",
    "\n",
    "# matrix = []\n",
    "# for index, value in enumerate(b):\n",
    "#     f = s.subs(x, Indexed('x', index))\n",
    "#     matrix.append(f.diff(Indexed('x',index)))\n",
    "\n",
    "#     matrix = Matrix(matrix)\n",
    "# display(matrix)\n",
    "\n",
    "diff = s.diff(Indexed('x',i))\n",
    "display(diff)\n",
    "# # diff = diff.subs(Indexed('x',0), x)\n",
    "display(simplify(diff.doit()))\n",
    "\n",
    "g = lambdify(x, diff)\n",
    "print(g(b))\n",
    "\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7d32c78-7dd3-4ed1-8f5b-e70bc1b21eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle {x}_{i}$"
      ],
      "text/plain": [
       "x[i]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5c3067b03643>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndexed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# s = s.subs(n, len(b))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "x, i, n = symbols(\"x i n\")\n",
    "s = Indexed('x',i)\n",
    "display(s)\n",
    "b = np.array([1, 2, 3, 4])\n",
    "# s = s.subs(n, len(b))\n",
    "display(s.doit())\n",
    "\n",
    "f = lambdify(x, s)\n",
    "print(f(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9351c4-bb37-4796-8b22-d611472e3dab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
