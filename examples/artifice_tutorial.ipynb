{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89c1e3e0-2440-4fd0-be13-758a4b65d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from artifice.network import NN\n",
    "from artifice.helper import apply_random_permutation, one_hot_encode, load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db144b9d-aba4-4236-b6d2-793c0c00c588",
   "metadata": {},
   "source": [
    "# Example on How to Build a Basic Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "829bb88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols, lambdify, Piecewise, Eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d52b79a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction:\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        # declare the variables of interest (x for activations, y and y_hat for loss)\n",
    "        self.y_pred, self.y_true = symbols(\"y_pred y_true\", real=True)\n",
    "        self.expression = None\n",
    "\n",
    "    def evaluate(self, y_pred, y_true, diff = False):\n",
    "\n",
    "        assert self.expression is not None, \"Loss function is not set!\"\n",
    "\n",
    "        if diff:\n",
    "            self.func = lambdify((self.y_pred, self.y_true), self.expression.diff(self.y_pred))\n",
    "        else:\n",
    "            self.func = lambdify((self.y_pred, self.y_true), self.expression)\n",
    "\n",
    "        return self.func(y_pred, y_true)\n",
    "\n",
    "\n",
    "class MeanSquaredError(LossFunction):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.expression = (self.y_pred - self.y_true)**2\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Mean Squared Error Loss Function: \\n{self.expression}'\n",
    "\n",
    "# class MeanSquaredError(LossFunction):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.expression = (self.y_pred - self.y_true)**2\n",
    "\n",
    "#         print(self.expression)\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "class ActivationFunction:\n",
    "\n",
    "    def __init__(self):\n",
    "        # declare the variables of interest (x for activations, y and y_hat for loss)\n",
    "        self.x = symbols(\"x\", real=True)\n",
    "        self.expression = None\n",
    "\n",
    "    def evaluate(self, x_val, diff = False):\n",
    "\n",
    "        assert self.expression is not None, \"Activation function is not set!\"\n",
    "\n",
    "        if diff:\n",
    "            self.func = lambdify((self.x), self.expression.diff(self.x))\n",
    "        else:\n",
    "            self.func = lambdify((self.x), self.expression)\n",
    "\n",
    "        return self.func(x_val)\n",
    "\n",
    "class Relu(ActivationFunction):\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.expression = Piecewise((0,self.x<0),(self.x, self.x>=0))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Relu Activation Function: \\n{self.expression}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9b1b1bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mean Squared Error Loss Function: \n",
       "(y_pred - y_true)**2"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MeanSquaredError()#.evaluate(1, 2, diff=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "14caa15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piecewise((0, x < 0), (x, True))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Relu Activation Function: \n",
       "Piecewise((0, x < 0), (x, True))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8d7a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Code written by William Bidle and Ilana Zane \"\"\"\n",
    "\n",
    "__version__ = \"dev\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from typing import List, Union, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class NN:\n",
    "\n",
    "    \"\"\"\n",
    "    Defines a neural network.\n",
    "\n",
    "    :param layer_sequence: A list containing the nodes per layer and correcponding activation\n",
    "    functions between layers.\n",
    "    :param loss_function: The desired loss function to be used.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer_sequence: list = None, loss_function: str = \"MSE\"):\n",
    "\n",
    "        # Load in the function library\n",
    "        self.activation_funcs_library = self.__load_func_libraries(\n",
    "            \"activation_funcs_library.txt\"\n",
    "        )\n",
    "\n",
    "        # Separate out the layer information (every other element)\n",
    "        layers = layer_sequence[::2]\n",
    "\n",
    "        # check that the declared number of nodes is an integer\n",
    "        if all(isinstance(item, int) for item in layers) is True:\n",
    "            # set the 'layers' property\n",
    "            layers = np.array(layer_sequence[::2], dtype=int)\n",
    "\n",
    "            # initialize the *weights* property based off of the desired layer sequence\n",
    "            self.weights = self.__initialize_weights(layers)\n",
    "\n",
    "        else:\n",
    "            # raise an exception if the input layer sequence is improperly defined\n",
    "            raise ValueError(\"Invalid Layer Sequence!\")\n",
    "\n",
    "        # separate out the activation function information\n",
    "        activation_funcs = layer_sequence[1::2]\n",
    "\n",
    "        # check that the activation functions are strings\n",
    "        if all(isinstance(item, str) for item in activation_funcs) is True:\n",
    "            # initialize the 'activation_funcs' property\n",
    "            self.activation_funcs = []\n",
    "\n",
    "            # initialize each declared activation functions between layers\n",
    "            for activation_func in activation_funcs:\n",
    "                self.activation_funcs.append(\n",
    "                    self.__init_func(self.activation_funcs_library, activation_func)\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            # raise an exception if the input layer sequence is improperly defined\n",
    "            raise ValueError(\"Invalid Layer Sequence!\")\n",
    "\n",
    "        # initialize the *loss_funcs_library* property\n",
    "        self.loss_funcs_library = self.__load_func_libraries(\"loss_funcs_library.txt\")\n",
    "\n",
    "        # initialize the *loss_func* property\n",
    "        self.loss_func = self.__init_func(self.loss_funcs_library, loss_function)\n",
    "\n",
    "        # initialize the *loss_func_label* property (used in plotting for now)\n",
    "        self.loss_func_label = loss_function\n",
    "\n",
    "        # initialize the *training_err* property (will be set later once the model is trained)\n",
    "        self.training_err = None\n",
    "\n",
    "    def __load_func_libraries(self, func_file: str) -> dict:\n",
    "\n",
    "        \"\"\"\n",
    "        Loads in dictionaries of available functions.\n",
    "\n",
    "        :param func_file: the filename containing the library of usable functions.\n",
    "        :returns func_library: a dictionary of the usable functions.\n",
    "        \"\"\"\n",
    "\n",
    "        # open the desired file\n",
    "        with open(\n",
    "            os.path.join(os.path.dirname(__file__), func_file), encoding=\"utf-8\"\n",
    "        ) as f:\n",
    "            data = f.read()\n",
    "\n",
    "        # reconstruct the data as a dictionary\n",
    "        func_library = json.loads(data)\n",
    "\n",
    "        return func_library\n",
    "\n",
    "    def __init_func(\n",
    "        self, func_library: dict, func_name: str\n",
    "    ) -> sympy.core.symbol.Symbol:\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize a function from a function library\n",
    "\n",
    "        :param func_library: A dictionary of the usable functions.\n",
    "        :param func_name: The name of the mathematical function to be initialized (e.g., 'sigmoid').\n",
    "        :returns expression: Symbolic mathematical representation of 'func_name'.\n",
    "        \"\"\"\n",
    "\n",
    "        # try to initialize the function\n",
    "        try:\n",
    "            expression = func_library[func_name]\n",
    "\n",
    "        # if the function doesn't exist within the function library, return an exception\n",
    "        except Exception as exc:\n",
    "            raise ValueError(\n",
    "                f\"Desired function '{func_name}' does not exist within the 'func_library.'\"\n",
    "            ) from exc\n",
    "\n",
    "        # declare the variables of interest (x for activations, y and y_hat for loss)\n",
    "        x, y, y_hat = sympy.symbols(\"x y y_hat\", real=True)\n",
    "\n",
    "        # parse throught the expression\n",
    "        expression = sympy.parse_expr(\n",
    "            expression, local_dict={\"x\": x, \"y\": y, \"y_hat\": y_hat}\n",
    "        )\n",
    "\n",
    "        return expression\n",
    "\n",
    "    def __eval_func(\n",
    "        self, expression: sympy.core.symbol.Symbol, vals: List[List], diff: bool = False\n",
    "    ) -> Union[float, np.ndarray]:\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize an activation function.\n",
    "\n",
    "        :param expression: Symbolic mathematical representation of 'func'.\n",
    "        :param vals: The values to evaluate 'expression' with - 1 sub-list for activation functions\n",
    "        (x information), 2 sub-lists for loss functions (y, y_hat information).\n",
    "        :param diff: Whether or not to evaluate the derivitive of 'expression' at 'vals'.\n",
    "        :returns result: evaluation of 'expression' at '_input_' - if diff = False -> Float,\n",
    "        if diff = True -> np.ndarray.\n",
    "        \"\"\"\n",
    "\n",
    "        # Evaluate Activation Functions\n",
    "        if expression in self.activation_funcs:\n",
    "\n",
    "            # the variable of interest\n",
    "            x = sympy.Symbol(\"x\", real=True)\n",
    "\n",
    "            # differentiate only if the 'diff' flag is True\n",
    "            if diff is True:\n",
    "                expression = expression.diff(x)\n",
    "\n",
    "            # allow the function to be evaluated from lists\n",
    "            func = sympy.lambdify(x, expression)\n",
    "\n",
    "            # evaluate the function at the given input\n",
    "            result = func(vals[0])\n",
    "\n",
    "        # Evaluate Loss Functions\n",
    "        else:\n",
    "\n",
    "            # the variables of interest\n",
    "            y, y_hat = sympy.symbols(\"y, y_hat\", real=True)\n",
    "\n",
    "            # differentiate only if the 'diff' flag is True\n",
    "            if diff is True:\n",
    "                expression = expression.diff(y)\n",
    "\n",
    "                # allow the function to be evaluated from lists\n",
    "                func = sympy.lambdify((y, y_hat), expression)\n",
    "\n",
    "                # evaluate the function at the given input\n",
    "                result = func(vals[0], vals[1])\n",
    "\n",
    "            else:\n",
    "\n",
    "                # allow the function to be evaluated from lists\n",
    "                func = sympy.lambdify((y, y_hat), expression)\n",
    "\n",
    "                # evaluate the function at the given input\n",
    "                result = sum(func(vals[0], vals[1]))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __initialize_weights(self, layers: np.ndarray) -> List[np.ndarray]:\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize the weights of the network.\n",
    "\n",
    "        :params layers: An array containing the layer information of the network.\n",
    "        :returns weights: List containing the 2D weight arrays between the different layers.\n",
    "        \"\"\"\n",
    "\n",
    "        layers_reorganized = np.flip(\n",
    "            layers.repeat(2)[1:-1].reshape(len(layers) - 1, 2), axis=1\n",
    "        )\n",
    "\n",
    "        # initialize the list of the weights between different layers\n",
    "        weights = []\n",
    "\n",
    "        for layer_reorganized in layers_reorganized:\n",
    "            # include bias vector with the '+ 1'\n",
    "            weight = np.random.randn(layer_reorganized[0], layer_reorganized[1] + 1)\n",
    "\n",
    "            # HE initialization for weights\n",
    "            weights.append(weight * np.sqrt(2 / layer_reorganized[1]))\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def __update_weights(\n",
    "        self, weights: List[np.ndarray], layer_values: List[List], _label_: np.ndarray\n",
    "    ) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "\n",
    "        \"\"\"\n",
    "        Update the weights of the network.\n",
    "\n",
    "        :params weights: List containing the 2D weight arrays between the different layers.\n",
    "        :params layer_values: List containing the values of each layer for a given input value.\n",
    "        :params _label_: Array representation for the current label, usually one hot ecoded.\n",
    "        :returns weight_updates: List of updated 2D weight arrays between the different layers.\n",
    "        :returns weights: List of original 2D weight arrays between the different layers.\n",
    "        \"\"\"\n",
    "\n",
    "        # get the list of desired activation functions\n",
    "        activations = self.activation_funcs\n",
    "\n",
    "        # make a copy of the weights so they aren't changed\n",
    "        weight_updates = weights.copy()\n",
    "\n",
    "        # blue in notes\n",
    "        blue = np.diag(\n",
    "            self.__eval_func(self.loss_func, [layer_values[-1], _label_], diff=True)\n",
    "        )\n",
    "\n",
    "        # need to add an extra component to input for bias\n",
    "        layer_output = np.dot(weights[-1], np.concatenate((layer_values[-2], [1])))\n",
    "\n",
    "        # red in notes\n",
    "        red = self.__eval_func(activations[-1], [layer_output], diff=True)\n",
    "\n",
    "        # index through each weight (work backwards)\n",
    "        for i in range(len(weights), 0, -1):\n",
    "\n",
    "            # pink in notes\n",
    "            pink = np.concatenate((layer_values[i - 1], [1]))\n",
    "\n",
    "            # first two terms in gradient\n",
    "            grad = np.matmul(blue, np.outer(red, pink))\n",
    "\n",
    "            # look forwards through each weight (only if there are forward weights)\n",
    "            for j in range(len(weights), i, -1):\n",
    "\n",
    "                # orange in notes\n",
    "                orange = np.transpose(weights[j - 1])\n",
    "\n",
    "                # green in notes\n",
    "                green = np.diag(\n",
    "                    self.__eval_func(\n",
    "                        activations[j - 1],\n",
    "                        [\n",
    "                            np.dot(\n",
    "                                weights[j - 2],\n",
    "                                np.concatenate((layer_values[j - 2], [1])),\n",
    "                            )\n",
    "                        ],\n",
    "                        diff=True,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # add on the bias vector to make sure dimensions work properly\n",
    "                bias_vec = np.ones((len(green), 1))\n",
    "\n",
    "                # incorperate the bias\n",
    "                green = np.hstack((green, bias_vec))\n",
    "\n",
    "                # now multiply the rest to grad\n",
    "                grad = np.matmul(green, np.matmul(orange, grad))\n",
    "\n",
    "            # record the change in weight\n",
    "            weight_updates[i - 1] = grad\n",
    "\n",
    "        return weight_updates, weights\n",
    "\n",
    "    def get_network_outputs(\n",
    "        self, weights: List[np.ndarray], _input_: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Initialize the weights of the network.\n",
    "\n",
    "        :params weights: List containing the 2D weight arrays between the different layers.\n",
    "        :params _inputs_: Input layer to the network.\n",
    "        :returns network_outputs: Output layer of the network.\n",
    "        \"\"\"\n",
    "\n",
    "        # get the list of desired activation functions\n",
    "        activations = self.activation_funcs\n",
    "\n",
    "        # add the first layer to the list\n",
    "        current_layer = _input_\n",
    "        network_outputs = [current_layer]\n",
    "\n",
    "        for index, weight in enumerate(weights):\n",
    "\n",
    "            # need to add an extra component to input for bias\n",
    "            layer_output = np.dot(weight, np.concatenate((current_layer, [1])))\n",
    "\n",
    "            current_layer = self.__eval_func(\n",
    "                activations[index], [layer_output], diff=False\n",
    "            )\n",
    "\n",
    "            network_outputs.append(current_layer)\n",
    "\n",
    "        return network_outputs\n",
    "\n",
    "    def compute_error(self, _result_: np.ndarray, _label_: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute the error between the neural network's output and expected value.\n",
    "\n",
    "        :params _result_: Output layer of the network.\n",
    "        :params _label_: Expected result.\n",
    "        :returns error: The error of the network.\n",
    "        \"\"\"\n",
    "        error = self.__eval_func(self.loss_func, [_result_, _label_])\n",
    "\n",
    "        return error\n",
    "\n",
    "    def train(  # pylint: disable=too-many-arguments, too-many-locals\n",
    "        self, x_train, y_train, batch_size=1, epochs=1, epsilon=1, visualize=False\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Train a model.\n",
    "\n",
    "        :params x_train:\n",
    "        :params y_train:\n",
    "        :params batch_size:\n",
    "        :params epochs:\n",
    "        :params epsilon:\n",
    "        :params visualize:\n",
    "        \"\"\"\n",
    "\n",
    "        weights = self.weights  # get the list of weights\n",
    "\n",
    "        error_list = []\n",
    "\n",
    "        counter = 0  # keep track of the current iteration\n",
    "\n",
    "        weights_list = (\n",
    "            {}\n",
    "        )  # create a dictionary to keep track of the weight updates (batch size)\n",
    "\n",
    "        # just a temporary blank array since training hasn't begun yet\n",
    "        for i in range(len(weights)):\n",
    "            weights_list[i] = []\n",
    "\n",
    "        for i in range(epochs):\n",
    "\n",
    "            # iterate through the inputs and labels\n",
    "            for _input_, _label_ in tqdm(\n",
    "                zip(x_train, y_train), total=len(x_train), desc=f\"Epoch {str(i + 1)}\"\n",
    "            ):\n",
    "\n",
    "                network_output = self.get_network_outputs(\n",
    "                    weights, _input_\n",
    "                )  # the current network output\n",
    "\n",
    "                error = self.compute_error(network_output[-1], _label_)\n",
    "\n",
    "                weight_updates, weights = self.__update_weights(\n",
    "                    weights, network_output, _label_\n",
    "                )\n",
    "\n",
    "                for j in range(len(weights)):\n",
    "                    weights_list[j].append(weight_updates[j])\n",
    "\n",
    "                counter += 1\n",
    "\n",
    "                if (counter) % batch_size == 0:\n",
    "                    for index, weight in enumerate(weights):\n",
    "\n",
    "                        weights[index] = weight - epsilon * np.average(\n",
    "                            np.array(weights_list[index]), axis=0\n",
    "                        )\n",
    "                        weights_list[index] = []\n",
    "\n",
    "                error_list.append(error)\n",
    "\n",
    "            self.weights = weights\n",
    "            self.training_err = error_list\n",
    "\n",
    "        if visualize is True:\n",
    "            _, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "            ax.plot(self.training_err)  # to visualize error over time\n",
    "\n",
    "            ax.set_xlabel(\"Training Sample\", fontsize=14)\n",
    "            ax.set_ylabel(f\"{self.loss_func_label} Error\", fontsize=14)\n",
    "\n",
    "            ax.grid(linestyle=\"--\")\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "    def evaluate(self, x_test):\n",
    "        \"\"\"\n",
    "        Evaluate a model.\n",
    "\n",
    "        :params x_test:\n",
    "        :returns results:\n",
    "        \"\"\"\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for _input_ in tqdm(x_test, desc=\"Evaluating Test Data\", total=len(x_test)):\n",
    "            # the current network output\n",
    "            network_output = self.get_network_outputs(self.weights, _input_)\n",
    "            results.append(network_output[-1])\n",
    "\n",
    "        return results\n",
    "\n",
    "    def save_model(self, out_dir: str, filename: str) -> None:\n",
    "        \"\"\"\n",
    "        Save a model.\n",
    "\n",
    "        :params filename:\n",
    "        \"\"\"\n",
    "\n",
    "        # Enforce trailing backslash to directory\n",
    "        out_dir = os.path.join(out_dir, \"\")\n",
    "\n",
    "        # Check if out_dir exists\n",
    "        if not os.path.exists(out_dir):\n",
    "            raise ValueError(f\"Invalid path: {out_dir}.\")\n",
    "\n",
    "        save_path = out_dir + filename\n",
    "\n",
    "        to_save = [\n",
    "            self.weights,\n",
    "            self.activation_funcs,\n",
    "        ]  # save both the activations and weights\n",
    "        with open(save_path, \"wb\") as fp:  # save the weights and activations\n",
    "            pickle.dump(to_save, fp)\n",
    "\n",
    "        print()\n",
    "        print(f\"Model saved at '{save_path}'\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014ad5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bf4052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e31bb35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c4ec9bc-543c-4561-9f8e-214fff4994fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m layer_sequence \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReLU\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m      2\u001b[0m loss_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSLE\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m nn \u001b[38;5;241m=\u001b[39m \u001b[43mNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Avaliable Class Properties\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation func library:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, nn\u001b[38;5;241m.\u001b[39mactivation_funcs_library, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 29\u001b[0m, in \u001b[0;36mNN.__init__\u001b[1;34m(self, layer_sequence, loss_function)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, layer_sequence: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, loss_function: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Load in the function library\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_funcs_library \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load_func_libraries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactivation_funcs_library.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Separate out the layer information (every other element)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     layers \u001b[38;5;241m=\u001b[39m layer_sequence[::\u001b[38;5;241m2\u001b[39m]\n",
      "Cell \u001b[1;32mIn[4], line 89\u001b[0m, in \u001b[0;36mNN.__load_func_libraries\u001b[1;34m(self, func_file)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03mLoads in dictionaries of available functions.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m:param func_file: the filename containing the library of usable functions.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m:returns func_library: a dictionary of the usable functions.\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# open the desired file\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m---> 89\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18;43m__file__\u001b[39;49m), func_file), encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     91\u001b[0m     data \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# reconstruct the data as a dictionary\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "layer_sequence = [1,'ReLU', 2, 'sigmoid', 3]\n",
    "loss_function = 'MSLE'\n",
    "\n",
    "nn = NN(layer_sequence, loss_function)\n",
    "\n",
    "# Avaliable Class Properties\n",
    "print('activation func library:\\n', nn.activation_funcs_library, '\\n')\n",
    "print('loss func library:\\n', nn.loss_funcs_library, '\\n')\n",
    "print('current weights:\\n', nn.weights, '\\n')\n",
    "print('current activation functions:\\n', nn.activation_funcs, '\\n')\n",
    "print('current loss function:\\n', nn.loss_func_label, ':', nn.loss_func, '\\n')\n",
    "print('traing error:\\n', nn.training_err, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55788985-57d6-4504-9ffd-1f23f78347c6",
   "metadata": {},
   "source": [
    "# Example of Classification with MNIST Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20391ec-6f05-4160-a3bb-92cc5baf0cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' For Dataset Usage '''\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5595f8-8d2c-4881-96ee-9415ce392954",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Training Data Prep ###########################\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = (x_train.astype('float32') / 255.).reshape(len(x_train), len(x_train[0])*len(x_train[0][0]))\n",
    "x_test = (x_test.astype('float32') / 255.).reshape(len(x_test), len(x_test[0])*len(x_test[0][0]))\n",
    "\n",
    "num_data_pts = 1000\n",
    "\n",
    "x_train = x_train[0:num_data_pts]\n",
    "y_train = one_hot_encode(y_train[0:num_data_pts])\n",
    "\n",
    "input_shape = x_train[0].size\n",
    "output_shape = y_train[0].size\n",
    "\n",
    "########################### Testing Data Prep ############################\n",
    "\n",
    "num_test_evals = 1000\n",
    "\n",
    "x_test = x_test[0:num_test_evals]\n",
    "y_test = one_hot_encode(y_test[0:num_test_evals])\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c0af97-696a-485b-a11e-e87d42d71eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## Build network ##############################\n",
    "\n",
    "# layer_sequence = [input_shape, 'ReLU', 392, 'ReLU', 196, 'ReLU', 98, 'ReLU', 49, 'sigmoid', output_shape]\n",
    "layer_sequence = [input_shape, 'ReLU', 100, 'sigmoid', output_shape] # initialize the layer sequences and corresponding activations\n",
    "\n",
    "loss_function = 'MSE' # declare the loss function\n",
    "\n",
    "nn = NN(layer_sequence, loss_function) # Build a model \n",
    "\n",
    "################################# Run it ##################################\n",
    "\n",
    "nn.train(x_train, y_train, epochs = 1, batch_size = 5, epsilon = 0.01, visualize = True)\n",
    "\n",
    "################################# Testing ##################################\n",
    "\n",
    "nn.evaluate(x_test)\n",
    "\n",
    "nn.save_model(out_dir=\"../../temp/saved_models\", filename=\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cc8c90-12cc-47bb-b765-fc645efeaaf1",
   "metadata": {},
   "source": [
    "# Comparison of Different Loss Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d38eb36-c7cb-4b59-8d62-523811be0952",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Training Data Prep ###########################\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = (x_train.astype('float32') / 255.).reshape(len(x_train), len(x_train[0])*len(x_train[0][0]))\n",
    "x_test = (x_test.astype('float32') / 255.).reshape(len(x_test), len(x_test[0])*len(x_test[0][0]))\n",
    "\n",
    "num_data_pts = 1000\n",
    "\n",
    "x_train = x_train[0:num_data_pts]\n",
    "y_train = One_Hot_Encode(y_train[0:num_data_pts])\n",
    "\n",
    "input_shape = x_train[0].size\n",
    "output_shape = y_train[0].size\n",
    "\n",
    "########################### Testing Data Prep ############################\n",
    "\n",
    "num_test_evals = 1000\n",
    "\n",
    "x_test = x_test[0:num_test_evals]\n",
    "y_test = One_Hot_Encode(y_test[0:num_test_evals])\n",
    "\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45030721-bd19-43a2-98ab-bc47a534dcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_sequence = [input_shape, 'ReLU', 392, 'ReLU', 196, 'ReLU', 98, 'ReLU', 49, 'sigmoid', output_shape]\n",
    "layer_sequence = [input_shape, 'ReLU', 100, 'sigmoid', output_shape] # initialize the layer sequences and corresponding activations\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "############################## Build network #############################\n",
    "##########################################################################\n",
    "loss_function = 'MSE'\n",
    "\n",
    "nn1 = NN(layer_sequence, loss_function) # Build model \n",
    "\n",
    "################################# Run it ##################################\n",
    "\n",
    "nn1.train(x_train, y_train, batch_size = 5, epsilon = 0.01, visualize = False)\n",
    "\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "\n",
    "##########################################################################\n",
    "############################## Build network #############################\n",
    "##########################################################################\n",
    "\n",
    "loss_function = 'MAE'\n",
    "\n",
    "nn2 = NN(layer_sequence, loss_function) # Build model \n",
    "\n",
    "################################# Run it ##################################\n",
    "\n",
    "nn2.train(x_train, y_train, batch_size = 5, epsilon = 0.01, visualize = False)\n",
    "\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "\n",
    "##########################################################################\n",
    "############################## Build network #############################\n",
    "##########################################################################\n",
    "\n",
    "loss_function = 'MAPE'\n",
    "\n",
    "nn3 = NN(layer_sequence, loss_function) # Build model \n",
    "\n",
    "################################# Run it ##################################\n",
    "\n",
    "nn3.train(x_train, y_train, batch_size = 5, epsilon = 0.01, visualize = False)\n",
    "\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "\n",
    "##########################################################################\n",
    "############################## Build network #############################\n",
    "##########################################################################\n",
    "\n",
    "loss_function = 'MSLE'\n",
    "\n",
    "nn4 = NN(layer_sequence, loss_function) # Build model \n",
    "\n",
    "################################# Run it ##################################\n",
    "\n",
    "nn4.train(x_train, y_train, batch_size = 5, epsilon = 0.01, visualize = False)\n",
    "\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd0b49-b8a0-46e5-86b8-a8fcaf972153",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize = (16,8))\n",
    "            \n",
    "ax[0][0].plot(nn1.training_err, label = '%s' % str(nn1.loss_func)) # to visualize error over time\n",
    "ax[0][0].set_xlabel('Training Sample', fontsize = 14)\n",
    "ax[0][0].set_ylabel('%s Error' % nn1.loss_func_label, fontsize = 14)\n",
    "ax[0][0].grid(linestyle = '--')\n",
    "ax[0][0].legend(fontsize = 12)\n",
    "\n",
    "ax[0][1].plot(nn2.training_err, label = '%s' % str(nn2.loss_func)) # to visualize error over time\n",
    "ax[0][1].set_xlabel('Training Sample', fontsize = 14)\n",
    "ax[0][1].set_ylabel('%s Error' % nn2.loss_func_label, fontsize = 14)\n",
    "ax[0][1].grid(linestyle = '--')\n",
    "ax[0][1].legend(fontsize = 12)\n",
    "\n",
    "ax[1][0].plot(nn3.training_err, label = '%s' % str(nn3.loss_func)) # to visualize error over time\n",
    "ax[1][0].set_xlabel('Training Sample', fontsize = 14)\n",
    "ax[1][0].set_ylabel('%s Error' % nn3.loss_func_label, fontsize = 14)\n",
    "ax[1][0].grid(linestyle = '--')\n",
    "ax[1][0].legend(fontsize = 12)\n",
    "\n",
    "ax[1][1].plot(nn4.training_err, label = '%s' % str(nn4.loss_func)) # to visualize error over time\n",
    "ax[1][1].set_xlabel('Training Sample', fontsize = 14)\n",
    "ax[1][1].set_ylabel('%s Error' % nn4.loss_func_label, fontsize = 14)\n",
    "ax[1][1].grid(linestyle = '--')\n",
    "ax[1][1].legend(fontsize = 12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d9477a-ed8f-4f21-ab5c-f209360e464c",
   "metadata": {},
   "source": [
    "# Example of Classification with MNIST (only 2 labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e755e439-5e32-4393-b5ca-0b74be6b7e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_number = 0\n",
    "second_number = 1\n",
    "\n",
    "################################# Params #################################\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "train_zeros = y_train == first_number\n",
    "train_ones = y_train == second_number\n",
    "\n",
    "x_train = np.array(list(x_train[train_zeros]) + list(x_train[train_ones]))\n",
    "y_train = np.array(list(y_train[train_zeros]) + list(y_train[train_ones]))\n",
    "\n",
    "x_train = (x_train.astype('float32') / 255.).reshape(len(x_train), len(x_train[0])*len(x_train[0][0]))\n",
    "\n",
    "x_train, y_train = unison_shuffled_copies(x_train, y_train)\n",
    "\n",
    "x_train, y_train = x_train[:1000], One_Hot_Encode(y_train[:1000])\n",
    "\n",
    "test_zeros = y_test == first_number\n",
    "test_ones = y_test == second_number\n",
    "\n",
    "x_test = np.array(list(x_test[test_zeros]) + list(x_test[test_ones]))\n",
    "y_test = np.array(list(y_test[test_zeros]) + list(y_test[test_ones]))\n",
    "\n",
    "x_test = (x_test.astype('float32') / 255.).reshape(len(x_test), len(x_test[0])*len(x_test[0][0]))\n",
    "\n",
    "x_test, y_test = unison_shuffled_copies(x_test, y_test)\n",
    "\n",
    "x_test, y_test = x_test[:1000], One_Hot_Encode(y_test[:1000])\n",
    "\n",
    "input_shape = x_train[0].size\n",
    "output_shape = y_train[0].size\n",
    "\n",
    "\n",
    "############################## Build network ##############################\n",
    "\n",
    "# layer_sequence = [input_shape, 'ReLU', 392, 'ReLU', 196, 'ReLU', 98, 'ReLU', 49, 'sigmoid', output_shape]\n",
    "layer_sequence = [input_shape, 'ReLU', 100, 'sigmoid', output_shape]\n",
    "\n",
    "loss_function = 'MSE'\n",
    "# loss_function = 'Binary Cross-Entropy'\n",
    "\n",
    "nn = NN(layer_sequence, loss_function) # Build model \n",
    "\n",
    "################################# Run it ##################################\n",
    "\n",
    "nn.train(x_train, y_train, batch_size = 5, epsilon = 0.1, visualize = True)\n",
    "\n",
    "################################# Testing ##################################\n",
    "\n",
    "nn.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40084eaa-a6d9-4b2c-bb9e-cfc599e7221c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb39de19-0868-40a5-8475-b70c5bd9518d",
   "metadata": {},
   "source": [
    "# Compare to TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dad32f-1cf8-412c-923d-4289dc88085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0f2a7f-493a-4c93-9f58-edf04f8194ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "zeros_train = y_train == 0\n",
    "x_train_zeros, y_train_zeros = list(x_train[zeros_train]), list(y_train[zeros_train])\n",
    "zeros_test = y_test == 0\n",
    "x_test_zeros, y_test_zeros = list(x_test[zeros_test]), list(y_test[zeros_test])\n",
    "\n",
    "ones_train = y_train == 1\n",
    "x_train_ones, y_train_ones = list(x_train[ones_train]), list(y_train[ones_train])\n",
    "ones_test = y_test == 1\n",
    "x_test_ones, y_test_ones = list(x_test[ones_test]), list(y_test[ones_test])\n",
    "\n",
    "\n",
    "x_train, y_train = np.array(x_train_zeros + x_train_ones), np.array(y_train_zeros + y_train_ones)\n",
    "x_test, y_test = np.array(x_test_zeros + x_test_ones), np.array(y_test_zeros + y_test_ones)\n",
    "\n",
    "\n",
    "# shuffle arrays\n",
    "\n",
    "x_train, y_train = unison_shuffled_copies(x_train, y_train)\n",
    "x_test, y_test = unison_shuffled_copies(x_test, y_test)\n",
    "\n",
    "x_train = (x_train / 255.)\n",
    "x_test = (x_test / 255.)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63279c3-99c7-4c32-9e29-dc3449fa275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(100, activation='relu'),\n",
    "  #tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(2, activation='sigmoid')\n",
    "])\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680832a6-11d0-43b6-8257-8ae8ba9d8dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4b8e9f-ffd2-475e-9255-e07fc46c6d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57296b4d-9b80-4694-bdef-13a4b1c83a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size= 5, epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab33783f417e33bc25adfd86bb21951daad33a199843654519bb68b1a101f331"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
